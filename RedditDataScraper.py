# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FgCSIWszEoJT1QLOZ9M-fk0b7weFAvqJ
"""

import pandas as pd
import os
from dotenv import load_dotenv
import asyncpraw

client_id = os.getenv("REDDIT_CLIENT_ID")
client_secret = os.getenv("REDDIT_CLIENT_SECRET")
user_agent = os.getenv("REDDIT_USER_AGENT")
username = os.getenv("REDDIT_USERNAME")
password = os.getenv("REDDIT_PASSWORD")
load_dotenv()
# Authentication
reddit = asyncpraw.Reddit(
    client_id= client_id,
    client_secret=client_secret,
    user_agent=user_agent,
    username=username,
    password=password
)

top_posts = {'ID':[],'URL':[],'Title':[],'Upvotes':[],'Comments':[]}

subreddit = await reddit.subreddit("restaurants")   # add a subreddit name here
async for submission in subreddit.top(time_filter="year",limit=25):
  top_posts["ID"].append(submission.id)
  top_posts["URL"].append(submission.url)
  top_posts["Title"].append(submission.title)
  top_posts["Upvotes"].append(submission.score)
  top_posts["Comments"].append(submission.num_comments)

df = pd.DataFrame.from_dict(top_posts)
df

df.to_csv('top-posts.csv', index=False) # this prints from dataframe to a CSV file

!pip install asyncpraw nest_asyncio

import asyncio
import nest_asyncio
import time
import pandas as pd
from datetime import datetime

import asyncpraw

nest_asyncio.apply()

reddit = asyncpraw.Reddit(
    client_id="E2ctZMStJyJvGAlQxpljeQ",
    client_secret="eCd2lzZsvrjGYSX2hr_755W1NHD8DQ",
    user_agent="Johan John ThomasSpare-Two9445",
    username="Spare-Two9445",
    password="Quickcreat0r*!"
)

async def safe_fetch(generator):
    """
    Wraps an async generator and, if a rate-limit error occurs,
    sleeps 5 seconds and retries.
    """
    while True:
        try:
            async for item in generator:
                yield item
            # If we exhaust the generator successfully, we break.
            break
        except Exception as e:
            if "RATELIMIT" in str(e).upper():
                print("Rate limit reached. Sleeping 5 seconds...")
                await asyncio.sleep(5)
            else:
                # Some other error - raise it
                raise e

# We'll collect rows as dicts, then create a DataFrame at the end.
all_data = []

# Helper function to append a single row (for post or comment) to all_data
def add_row(content, category, upvotes, post_type, date):
    """
    Adds a row to our in-memory dataset.
      content: str
      category: str (queried, relevant, controversial, top, new, rising)
      upvotes: int
      post_type: str (post or comment)
      date: UTC float or ISO-string
    """
    all_data.append({
        'content': content,
        'category': category,
        'upvotes': upvotes,
        'type': post_type,
        'date': date
    })

async def fetch_submission_and_comments(submission, category):
    """
    Given a submission (post) and a category label (queried / relevant / etc),
    fetch the post data and all comments, then store them in all_data.
    """
    # Convert created_utc (float) into a human-readable string or just store the float
    post_date = datetime.utcfromtimestamp(submission.created_utc).isoformat()

    # Add the submission itself as a 'post'
    post_content = ""
    # Combine title + selftext, if you prefer
    if submission.title:
        post_content += submission.title
    if submission.selftext:
        post_content += "\n" + submission.selftext

    add_row(content=post_content.strip(),
            category=category,
            upvotes=submission.score,
            post_type="post",
            date=post_date)

    # Load all comments. This can be large, so be cautious in real usage.
    try:
        # Replace more comments to get the full tree.
        # limit=None tries to replace all MoreComments.
        await submission.comments.replace_more(limit=None)
    except Exception as e:
        if "RATELIMIT" in str(e).upper():
            print("Rate limit hit while expanding comments. Sleeping 5s and retrying...")
            await asyncio.sleep(5)
            # Attempt one more time
            await submission.comments.replace_more(limit=None)
        else:
            # Some other error
            raise e

    # Now iterate over all comments
    for comment in submission.comments.list():
        comment_date = datetime.utcfromtimestamp(comment.created_utc).isoformat()
        add_row(content=comment.body,
                category=category,
                upvotes=comment.score,
                post_type="comment",
                date=comment_date)

# --- STEP 1: Search for subreddits with the word "Samsung" ---
async def step1_collect_subreddits():
    found_subreddits = set()

    print("STEP 1: Searching subreddits by name for 'Samsung'...")
    # We wrap in safe_fetch so we handle rate-limit errors
    async for sub in safe_fetch(reddit.subreddits.search("Samsung", limit=50)):
        found_subreddits.add(sub.display_name)

    return found_subreddits

# --- STEP 2: Search all of reddit for the word "Samsung" and gather subreddits ---
async def step2_collect_subs_from_posts(existing_subs):
    print("STEP 2: Searching all posts (r/all) for 'Samsung'...")
    subreddit_all = await reddit.subreddit("all")

    # We'll do a standard search up to 1000 results
    async for post in safe_fetch(subreddit_all.search("Samsung", limit=1000, sort='date')):
        existing_subs.add(post.subreddit.display_name)

    return existing_subs

# --- STEP 3: For each subreddit, search "Samsung" (limit=100 posts) ---
async def step3_query_subs_for_samsung(sub_list):
    print("STEP 3: Query each subreddit in our set for 'Samsung'...")

    for sub_name in sub_list:
        try:
            subreddit = await reddit.subreddit(sub_name)
            print(f"  -> Searching r/{sub_name} for 'Samsung' (limit=100)")

            # We only do 100 posts (the user requested)
            async for submission in safe_fetch(subreddit.search("Samsung", limit=100)):
                await fetch_submission_and_comments(submission, category="queried")

        except Exception as e:
            # If a sub is private or banned or something else, we might get an error
            print(f"  !! Error accessing r/{sub_name}: {e}")
            # Continue on to the next subreddit
            continue

# --- STEP 4: Now fetch 100 posts (and comments) in each category from each subreddit ---
async def step4_fetch_posts_in_categories(sub_list):
    print("STEP 4: Fetch 100 posts in each category from each subreddit, no search term...")

    # Map actual function to the descriptive category name
    # The user specifically said: relevant (hot), controversial, top, new, rising
    categories = {
        'hot': 'relevant',
        'controversial': 'controversial',
        'top': 'top',
        'new': 'new',
        'rising': 'rising'
    }

    for sub_name in sub_list:
        try:
            subreddit = await reddit.subreddit(sub_name)
            print(f"  -> Gathering from r/{sub_name}")

            for cat_func, cat_label in categories.items():
                print(f"     Category: {cat_label} (limit=100)")
                # e.g. subreddit.hot(limit=100), subreddit.controversial(limit=100), etc.
                cat_method = getattr(subreddit, cat_func)

                async for submission in safe_fetch(cat_method(limit=100)):
                    await fetch_submission_and_comments(submission, category=cat_label)

        except Exception as e:
            print(f"  !! Error accessing r/{sub_name}: {e}")
            continue

async def main_pipeline():
    # STEP 1
    subreddits_found = await step1_collect_subreddits()
    print(f"Found {len(subreddits_found)} subreddits from Step 1.")

    # STEP 2
    subreddits_found = await step2_collect_subs_from_posts(subreddits_found)
    print(f"After Step 2, we have {len(subreddits_found)} unique subreddits.")

    # STEP 3
    await step3_query_subs_for_samsung(subreddits_found)

    # STEP 4
    await step4_fetch_posts_in_categories(subreddits_found)

    # DONE: Convert to DataFrame
    df = pd.DataFrame(all_data)
    print(f"Pipeline finished. Collected {len(df)} rows.")
    df.head(10)

    # Optionally save to CSV
    df.to_csv('reddit_samsung_data.csv', index=False)
    print("Data saved to reddit_samsung_data.csv")

loop = asyncio.get_event_loop()
loop.run_until_complete(main_pipeline())

print("All steps complete.")